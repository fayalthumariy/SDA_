from openai import OpenAI
from langchain.prompts import ChatPromptTemplate
import os
from concurrent.futures import ThreadPoolExecutor, as_completed


class SummarizeChunk:
    def __init__(self, text_chunk, temperature1=.2, temperature2=.1, max_token=500):
        self.text_chunk = text_chunk
        self.client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
        self.temperature1 = temperature1
        self.temperature2 = temperature2
        self.max_token = max_token

    def generate_summary_ar(self, text):
        """
ููุฎูุต ููุท ุงูุนูุงุตุฑ ุงูุชุงููุฉ ูู ุฃู ูุต ุนุฑุจู:
- ูุทุงู ุนูู ุงููุดุฑูุน
- ุจุฑูุงูุฌ ุงูุนูู/ุฎุทุฉ ุงูุชูููุฐ
- ููุงู ุชูููุฐ ุงูุฃุนูุงู
- ุฌุฏูู ุงููููุงุช ูุงูุฃุณุนุงุฑ
ุงููุงุชุฌ: ููุฎุต ุนุฑุจู ุฑุณูู ูู 8 ุฅูู 10 ุฌูู. ููุชุจ "ุบูุฑ ูุฐููุฑ" ุนูุฏ ุบูุงุจ ุฃู ุนูุตุฑ.
        """
        if not text or len(text.strip()) == 0:
            return None

        max_chars = 15000
        if len(text) > max_chars:
            text = text[:max_chars]

        summary_prompt = ChatPromptTemplate.from_template("""
        ุฃูุช ูุณุงุนุฏ ูุชุฎุตุต ูู ุชูุฎูุต ูุณุชูุฏุงุช ุงูููุงูุตุงุช ุจุงููุบุฉ ุงูุนุฑุจูุฉ.

        ุงููุฏู: ุฅูุชุงุฌ ููุฎุต ุฑุณูู ุจุงููุบุฉ ุงูุนุฑุจูุฉ ุงููุตุญู ููููู ูู 8 ุฅูู 10 ุฌูู ูุชุตูุฉุ ูุฑููุฒ ุญุตุฑุงู ุนูู:
        1) ูุทุงู ุนูู ุงููุดุฑูุนุ
        2) ุจุฑูุงูุฌ ุงูุนูู ุฃู ุฎุทุฉ ุงูุชูููุฐุ
        3) ููุงู ุชูููุฐ ุงูุฃุนูุงูุ
        4) ุฌุฏูู ุงููููุงุช ูุงูุฃุณุนุงุฑ (ุฅู ููุฌุฏ).

        ููุงุนุฏ ุตุงุฑูุฉ:
        - ูุง ุชุถู ุฃู ูุนูููุงุช ุนุงูุฉ ุฃู ูุชูุฑุฑุฉ ูุซู "ูุฌุจ ุนูู ููุฏูู ุงูุนุฑูุถ" ุฃู "ูููุงู ููุดุฑูุท ูุงูููุงุตูุงุช".
        - ูุง ุชุฎุชูู ุฃู ูุนูููุฉ ุบูุฑ ููุฌูุฏุฉ ูู ุงููุต. ุนูุฏ ุบูุงุจ ูุนูููุฉ ุงูุชุจ: "ุบูุฑ ูุฐููุฑ".
        - ูุง ุชูุฑูุฑ ููุณ ุงูููุฑุฉ ุฃู ุงูููุฑุฉ ูุฑุชูู.
        - ุงูุชุฒู ุจุซูุงูู ุฅูู ุนุดุฑ ุฌูู ูุงููุฉ ูุชุฑุงุจุทุฉ.
        - ุงุณุชุฎุฏู ูุบุฉ ุฑุณููุฉ ููุงุถุญุฉุ ุฏูู ุนูุงููู ุฃู ููุงุท.

        ุงููุต:
        {text}

        ุฃูุชุฌ ุงูุขู 8โ10 ุฌูู ุชุบุทู ุงูุนูุงุตุฑ ุงูุฃุฑุจุนุฉ ุจุงูุชุฑุชูุจ ุงูููุทูู (ูุทุงู ุงูุนูู โ ุงูุจุฑูุงูุฌ โ ุงูููุงู โ ุฌุฏูู ุงููููุงุช ูุงูุฃุณุนุงุฑ).
        ูุง ุชุถู ุนูุงููู ุฃู ูููุฉ "ุงูููุฎุต"ุ ููุท ุงูุฌูู ุงูููุงุฆูุฉ.
        """)

        filled_prompt = summary_prompt.format(text=text)

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system",
                     "content": "ูุณุงุนุฏ ูุชุฎุตุต ูู ุชูุฎูุต ูุณุชูุฏุงุช ุงูููุงูุตุงุช ุจุงููุบุฉ ุงูุนุฑุจูุฉุ ุฏููู ูุบูุฑ ูููููููู."},
                    {"role": "user", "content": filled_prompt}
                ],
                temperature=self.temperature1,
                max_tokens=self.max_token
            )

            summary = response.choices[0].message.content.strip()

            sentences = [s for s in summary.replace("ุ", ".").split(".") if s.strip()]
            if len(sentences) < 8:
                filled_prompt_retry = filled_prompt + "\n\nุชุฐููุฑ: ูุฌุจ ุฃู ูููู ุงูููุฎุต 8โ10 ุฌูููู ูุงููุฉ. ุงูุชุจ \"ุบูุฑ ูุฐููุฑ\" ููุนูุงุตุฑ ุงูุบุงุฆุจุฉ."
                response2 = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system",
                         "content": "ูุณุงุนุฏ ูุชุฎุตุต ูู ุชูุฎูุต ูุณุชูุฏุงุช ุงูููุงูุตุงุช ุจุงููุบุฉ ุงูุนุฑุจูุฉุ ุฏููู ูุบูุฑ ูููููููู."},
                        {"role": "user", "content": filled_prompt_retry}
                    ],
                    temperature=self.temperature2,
                    max_tokens=self.max_token
                )
                summary = response2.choices[0].message.content.strip()

            if "ุงูููุฎุต:" in summary:
                summary = summary.split("ุงูููุฎุต:", 1)[-1].strip()

            return summary

        except Exception as e:
            print(f"โ๏ธ Error during summarization: {e}")
            return None

    def summarize_chunks_ar_parallel(self, chunks, max_workers=6):
        def _work(idx, chunk):
            summary = self.generate_summary_ar(chunk)
            return idx, chunk, summary

        if not chunks:
            return []

        max_workers = max(1, min(max_workers, len(chunks)))
        results = [None] * len(chunks)

        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = {ex.submit(_work, i, ch): i for i, ch in enumerate(chunks)}
            total = len(futures)
            done_count = 0

            for fut in as_completed(futures):
                i, chunk, summary = fut.result()
                results[i] = (chunk, summary)
                done_count += 1
                print(f"\n๐น Summarized chunk {done_count}/{total} (index {i})")

        return results

    def combine_all_summarized_chunk(self, summaries):
        combined = ''

        for item in summaries:
            if isinstance(item, tuple):
                summary = item[1]
            else:
                summary = item

            if summary:
                combined = f'{combined}\n\n{summary}'

        return combined.strip()

"""
Main Pipeline - RFP Processing System
Complete RFP processing from start to finish
"""

from pdf_handler import HandlePDF
from chunker import Chunker
from summarize_chunk import SummarizeChunk
from company_info_extractor_original import process_company
import os
import json

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Folders
PDF_FOLDER = './pdfs/RFP'
OUTPUT_FOLDER = './outputs'

# Company website
COMPANY_WEBSITE = "https://rnec.sa/"

# Create output folder
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

print("=" * 60)
print(" ğŸ”¹ Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© RFP ğŸ”¹")
print("=" * 60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Read PDF files
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

all_rfps = os.listdir(PDF_FOLDER)
print(f"\n Ø¹Ø¯Ø¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø§Ù‚ØµØ§Øª: {len(all_rfps)}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Process each file
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

for i, each_rfp in enumerate(all_rfps, 1):
    print(f"\n{'=' * 60}")
    print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù {i}/{len(all_rfps)}: {each_rfp}")
    print(f"{'=' * 60}\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Stage 1: Extract text from PDF
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        print(f"ğŸ“„ [1/5] Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù€ PDF ...")
        pdf_path = os.path.join(PDF_FOLDER, each_rfp)
        pdf_handler = HandlePDF(pdf_path)
        extracted_text = pdf_handler.extract_text()

        if not extracted_text or len(extracted_text.strip()) == 0:
            print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {each_rfp} Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ")
            continue

        print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(extracted_text)} Ø­Ø±ÙÙ‹Ø§\n")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ: {str(e)}\n")
        continue

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Stage 2: Clean and chunk text
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        print(f" [2/5] ØªÙ†Ø¸ÙŠÙ ÙˆØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ ...")

        chunker = Chunker()
        cleaned_text = chunker.clean_text(extracted_text)
        chunks = chunker.chunk_text(cleaned_text)

        print(f"âœ… ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {len(chunks)} Ø¬Ø²Ø¡\n")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªÙ‚Ø³ÙŠÙ…: {str(e)}\n")
        continue

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Stage 3: Summarize chunks
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Stage 3: Summarize chunks
    try:
        print(f"ğŸ“ [3/5] ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ ...")

        summarizer = SummarizeChunk(chunks)


        print(f"   â³ ØªÙ„Ø®ÙŠØµ {len(chunks)} Ù‚Ø·Ø¹Ø© Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ...")
        results = summarizer.summarize_chunks_ar_parallel(chunks, max_workers=6)


        summaries = [summary for _, summary in results if summary]

        print(f"\nâœ… ØªÙ… ØªÙ„Ø®ÙŠØµ {len(summaries)}/{len(chunks)} Ø£Ø¬Ø²Ø§Ø¡")

        # merge
        combined_summary = summarizer.combine_all_summarized_chunk(results)
        print(f"âœ… Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {len(combined_summary)} Ø­Ø±Ù\n")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {str(e)}\n")
        combined_summary = ""
        continue

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Stage 4: Fetch company info (once only)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if i == 1:
        try:
            print(f"ğŸ¢ [4/5] Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ© Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ...")

            company_data = process_company(COMPANY_WEBSITE)

            if company_data:
                print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ© Ø¨Ù†Ø¬Ø§Ø­\n")

                company_file = os.path.join(OUTPUT_FOLDER, "company_info.json")
                with open(company_file, 'w', encoding='utf-8') as f:
                    json.dump(company_data, f, ensure_ascii=False, indent=2)

                print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ© ÙÙŠ: {company_file}\n")
            else:
                print(f"âš ï¸ ØªØ¹Ø°Ø± Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ©\n")
                company_data = {}

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ©: {str(e)}\n")
            company_data = {}

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Stage 5: Save results
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        print(f"ğŸ’¾ [5/5] Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ...")

        base_name = each_rfp.replace('.pdf', '')

        summary_file = os.path.join(OUTPUT_FOLDER, f"summary_{base_name}.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ”¹ Ù…Ù„Ø®Øµ RFP: {each_rfp}\n\n")
            f.write(combined_summary)

        print(f"   âœ… Ø§Ù„Ù…Ù„Ø®Øµ â†’ {summary_file}")

        cleaned_file = os.path.join(OUTPUT_FOLDER, f"cleaned_{base_name}.txt")
        with open(cleaned_file, 'w', encoding='utf-8') as f:
            f.write(cleaned_text)

        print(f"   âœ… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù â†’ {cleaned_file}")

        chunks_file = os.path.join(OUTPUT_FOLDER, f"chunks_{base_name}.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump({"chunks": chunks, "count": len(chunks)}, f, ensure_ascii=False, indent=2)

        print(f"   âœ… Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ â†’ {chunks_file}")

        print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ: {OUTPUT_FOLDER}\n")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {str(e)}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Finish
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "=" * 60)
print("ğŸ‰ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
print("=" * 60)

print(f"\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
print(f"   - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ¹Ø§Ù„Ø¬Ø©: {len(all_rfps)}")
print(f"   - Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {OUTPUT_FOLDER}")
print(f"   - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ©: company_info.csv & company_info.json")


